# app/core/premium.py
from __future__ import annotations

import os
import re
import html
from typing import List, Optional, Tuple
from textwrap import dedent
from loguru import logger

from app.core.llm_client import chat
from app.core.llm_router import Feature
from app.core.telegram_html import sanitize_tg_html

# --- –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤) ---
try:
    import pymorphy2  # type: ignore
    _MORPH = pymorphy2.MorphAnalyzer()
except Exception:
    _MORPH = None  # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ñ–æ–ª–ª–±–µ–∫: –±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏, –Ω–æ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç

# =========================================================
# –ü—Ä–µ–º–∏—É–º-–∞–Ω–∞–ª–∏–∑ (LLM) + –¥–µ–º–æ-—à–∞–±–ª–æ–Ω
# =========================================================

def _demo_template(dream_text: str, warn: Optional[str] = None) -> str:
    bullets: List[str] = []
    if dream_text.strip():
        bullets.append("üìñ <b>–ö—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑</b>\n‚Ä¢ " + dream_text.strip())

    bullets.append(
        "üîë <b>–°–∏–º–≤–æ–ª—ã –∏ –º–æ—Ç–∏–≤—ã</b>\n"
        "‚Ä¢ –í—ã–¥–µ–ª–∏–º –∫–ª—é—á–µ–≤—ã–µ –æ–±—Ä–∞–∑—ã –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Å–Ω–∞ (–≤ –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏ ‚Äî –≥–∏–±–∫–∏–π NLP-–∞–Ω–∞–ª–∏–∑)."
    )
    bullets.append(
        "üé≠ <b>–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω</b>\n"
        "‚Ä¢ –û–ø—Ä–µ–¥–µ–ª–∏–º –≤–µ–¥—É—â–∏–µ —ç–º–æ—Ü–∏–∏ –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã."
    )
    bullets.append(
        "üß† <b>–í–æ–∑–º–æ–∂–Ω—ã–π —Å–º—ã—Å–ª</b>\n"
        "‚Ä¢ –ì–∏–ø–æ—Ç–µ–∑–∞ –æ —Ç–µ–º–µ —Å–Ω–∞ —Å –æ–ø–æ—Ä–æ–π –Ω–∞ –∞—Ä—Ö–µ—Ç–∏–ø—ã –∏ —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."
    )
    bullets.append(
        "‚úÖ <b>–®–∞–≥–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏</b>\n"
        "‚Ä¢ 1‚Äì3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —à–∞–≥–∞, –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ–¥ —Å—é–∂–µ—Ç —Å–Ω–∞."
    )

    footer = "\n<i>(–î–µ–º–æ –ø—Ä–µ–º–∏—É–º-–∞–Ω–∞–ª–∏–∑–∞. –ü–æ—Å–ª–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ChatGPT API –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Ä–∞–∑–±–æ—Ä.)</i>"
    if warn:
        footer = f"\n<i>{warn}</i>" + footer

    return "<b>–ü—Ä–µ–º–∏—É–º-—Ä–∞–∑–±–æ—Ä —Å–Ω–∞</b>\n\n" + "\n\n".join(bullets) + footer


# --- —Å—Ç—Ä–∞—Ö–æ–≤–æ—á–Ω–∞—è —á–∏—Å—Ç–∫–∞ HTML –ø–æ–¥ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è Telegram ---
_ALLOWED = {"b", "i"}
_BR_RE = re.compile(r"(?is)<\s*br\s*/?\s*>")
_TAG_RE_ANY = re.compile(r"</?([a-zA-Z0-9]+)(?:\s[^>]*)?>")

def _ensure_tg_html(s: str) -> str:
    # 1) —Ç–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã –ø–µ—Ä–µ–Ω–æ—Å–∞–º
    s = _BR_RE.sub("\n", s)
    # <p>...</p> -> –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç —Å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π –º–µ–∂–¥—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏
    s = re.sub(r"(?is)<\s*p\s*>", "", s)
    s = re.sub(r"(?is)<\s*/\s*p\s*>", "\n\n", s)
    # <ul>/<li> -> –±—É–ª–ª–µ—Ç—ã
    s = re.sub(r"(?is)<\s*ul\s*>", "", s)
    s = re.sub(r"(?is)<\s*/\s*ul\s*>", "\n", s)
    s = re.sub(r"(?is)<\s*li\s*>", "‚Ä¢ ", s)
    s = re.sub(r"(?is)<\s*/\s*li\s*>", "\n", s)

    # 2) —É–¥–∞–ª—è–µ–º –≤—Å—ë, –∫—Ä–æ–º–µ <b>/<i>
    def _keep_or_strip(m: re.Match) -> str:
        tag = (m.group(1) or "").lower()
        return m.group(0) if tag in _ALLOWED else ""
    s = _TAG_RE_ANY.sub(_keep_or_strip, s)

    # 3) –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    s = re.sub(r"\n{3,}", "\n\n", s).strip()
    return s


def premium_analysis(dream_text: str) -> str:
    """
    –ü—Ä–µ–º–∏—É–º-–∞–Ω–∞–ª–∏–∑ —Å–Ω–∞ —á–µ—Ä–µ–∑ –Ω–∞—à LLM-—Ä–æ—É—Ç–µ—Ä.
    –£–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è PREMIUM_MODE=api|stub (stub ‚Äî –¥–µ–º–æ-–æ—Ç–≤–µ—Ç –±–µ–∑ LLM).
    """
    mode = os.getenv("PREMIUM_MODE", "api").lower()
    logger.debug(f"[premium] mode={mode}")

    if mode != "api":
        return _demo_template(dream_text)

    system = dedent("""
    –¢—ã ‚Äî –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Å–Ω–æ–≤–∏–¥–µ–Ω–∏—è–º. –ö—Ä–∞—Ç–∫–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–Ω.
    –í—ã–≤–æ–¥–∏ –ì–û–¢–û–í–´–ô HTML –¥–ª—è Telegram: —Ä–∞–∑–¥–µ–ª—ã —Å —ç–º–æ–¥–∑–∏, –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—É–Ω–∫—Ç—ã.

    –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Ä–∞–∑–¥–µ–ª—ã –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ):
    1) üìñ <b>–ö—Ä–∞—Ç–∫–∏–π –ø–µ—Ä–µ—Å–∫–∞–∑</b> ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    2) üîë <b>–°–∏–º–≤–æ–ª—ã –∏ –º–æ—Ç–∏–≤—ã</b> ‚Äî 2‚Äì5 –ø—É–Ω–∫—Ç–æ–≤. –ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –≤ –≤–∏–¥–µ:
       ¬´‚Ä¢ <—ç–º–æ–¥–∑–∏?> <–°–∏–º–≤–æ–ª> ‚Äî –∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ 3‚Äì8 —Å–ª–æ–≤¬ª.
    3) üé≠ <b>–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω</b> ‚Äî 2‚Äì4 –ø—É–Ω–∫—Ç–∞. –ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –¢–û–ß–ù–û –∫–∞–∫ –≤ ¬´–°–∏–º–≤–æ–ª–∞—Ö¬ª:
       ¬´‚Ä¢ <—ç–º–æ–¥–∑–∏?> <–≠–º–æ—Ü–∏—è> ‚Äî –∫—Ä–∞—Ç–∫–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ 3‚Äì8 —Å–ª–æ–≤¬ª.
       –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ <–≠–º–æ—Ü–∏—è>: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä: —Ä–∞–¥–æ—Å—Ç—å, —Ç—Ä–µ–≤–æ–≥–∞,
       —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –∏–Ω—Ç–µ—Ä–µ—Å). –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –º–æ–¥–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ (¬´–≤–æ–∑–º–æ–∂–Ω–æ¬ª, ¬´—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ¬ª,
       ¬´–∫–∞–∂–µ—Ç—Å—è¬ª –∏ —Ç.–ø.) –∏ –Ω–µ –ø–∏—à–∏ –ø–µ—Ä–µ—á–Ω–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é ‚Äî –∫–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π.
    4) üß† <b>–í–æ–∑–º–æ–∂–Ω—ã–π —Å–º—ã—Å–ª</b> ‚Äî 1 –∞–±–∑–∞—Ü.
    5) ‚úÖ <b>–®–∞–≥–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏</b> ‚Äî 3‚Äì5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤.

    –ù–∏–∫–∞–∫–∏—Ö –¥–∏—Å–∫–ª–µ–π–º–µ—Ä–æ–≤ –≤ –∫–æ–Ω—Ü–µ (–∏—Ö –¥–æ–±–∞–≤–∏—Ç –±–æ—Ç). –ù–∏–∫–∞–∫–∏—Ö –ø—Ä–æ—Å—å–± –æ –º–µ–¥–ø–æ–º–æ—â–∏.
    –Ø–∑—ã–∫ ‚Äî —Ä—É—Å—Å–∫–∏–π.
    –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π HTML-—Ç–µ–≥–∏ <br>, <ul>, <li>, <p>. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –∏ —Ç–µ–≥–∏ <b> –∏ <i>.
    """).strip()

    user = (dream_text or "").strip() or \
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–æ–Ω. –î–∞–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º—è–≥–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."

    messages = [
        {"role": "system", "content": system},
        {"role": "user",   "content": user},
    ]

    try:
        html_out = chat(Feature.DREAM, messages, temperature=0.7)
        html_out = sanitize_tg_html(html_out)
        html_out = _ensure_tg_html(html_out)  # —Å—Ç—Ä–æ–≥–∞—è —á–∏—Å—Ç–∫–∞ –ø–æ–¥ Telegram HTML
        # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø–æ–¥—Ä–µ–∂–µ–º –æ–ø–∞—Å–Ω—ã–µ —Ç–µ–≥–∏
        html_out = html_out.replace("<script", "&lt;script").replace("</script>", "&lt;/script&gt;")
        return html_out
    except Exception as e:
        logger.exception("premium_analysis failed")
        return _demo_template(dream_text, warn=f"(–û—à–∏–±–∫–∞ LLM: {e})")

# =========================================================
# –ü–∞—Ä—Å–µ—Ä –¥–ª—è –≤—ã—Ç–∞—Å–∫–∏–≤–∞–Ω–∏—è symbols/emotions –∏–∑ –ø—Ä–µ–º–∏—É–º-–æ—Ç–≤–µ—Ç–∞
# + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–æ—Ü–∏–π —á–µ—Ä–µ–∑ pymorphy2
# =========================================================

_TAG_RE = re.compile(r"</?(?:b|i)>", re.IGNORECASE)

def _strip_tg_html(s: str) -> str:
    """–£–±–∏—Ä–∞–µ–º <b>/<i>, –∑–∞–º–µ–Ω—è–µ–º <br> –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã, –¥–µ–∫–æ–¥–∏—Ä—É–µ–º HTML-entity, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã/–ø—Ä–æ–±–µ–ª—ã."""
    s = _TAG_RE.sub("", s)
    s = re.sub(r"(?i)<\s*br\s*/?\s*>", "\n", s)  # –≤–∞–∂–Ω–æ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    s = html.unescape(s)
    s = s.replace("\r", "")
    s = re.sub(r"[ \t]+\n", "\n", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def _find_section(text: str, titles: List[str]) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ–∫—Ü–∏–∏ –æ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞/–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏/–∫–æ–Ω—Ü–∞.
    titles ‚Äî –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, —ç–º–æ–¥–∑–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã).
    """
    title_re = r"|".join([re.escape(t) for t in titles])
    pat = re.compile(
        rf"(?mi)^(?:[\W_]*\s*)({title_re})\s*:?\s*\n+(.*?)(?=\n\s*(?:[^\n]*–°–∏–º–≤–æ–ª—ã|[^\n]*–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω|[^\n]*–í–æ–∑–º–æ–∂–Ω—ã–π —Å–º—ã—Å–ª|[^\n]*–®–∞–≥–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏|[^\n]*–û–±—â–∏–π –≤—ã–≤–æ–¥|$))",
        re.DOTALL,
    )
    m = pat.search(text)
    return m.group(2).strip() if m else ""

def _split_bullets(block: str) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ–º –±–ª–æ–∫ –Ω–∞ –ø—É–Ω–∫—Ç—ã: —Å—Ç—Ä–æ–∫–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å '-', '‚Ä¢', '‚Äî', —ç–º–æ–¥–∑–∏ –∏ —Ç.–ø."""
    lines = [ln.strip(" \t") for ln in block.split("\n")]
    items: List[str] = []
    for ln in lines:
        if not ln:
            continue
        if re.match(r"^[\-\‚Äì‚Äî‚Ä¢\*\u2022\ufe0f\W]\s*", ln):
            items.append(re.sub(r"^[\-\‚Äì‚Äî‚Ä¢\*\u2022\ufe0f\W]+\s*", "", ln))
        else:
            if items and (ln and (ln[0].islower() or ln[0].isdigit())):
                items[-1] += " " + ln
    return items

def _clean_phrase(s: str) -> str:
    s = s.strip(" .,!?:;‚Äû‚Äú¬´¬ª\"'()[]{}")
    s = re.sub(r"\s{2,}", " ", s)
    return s

def _left_before_dash(s: str) -> str:
    """–ë–µ—Ä—ë–º –ª–µ–≤—É—é —á–∞—Å—Ç—å –¥–æ —Ç–∏—Ä–µ ‚Äî —á–∞—Å—Ç–æ —Å–ø—Ä–∞–≤–∞ –∏–¥—ë—Ç –ø–æ—è—Å–Ω–µ–Ω–∏–µ."""
    parts = re.split(r"\s+[‚Äì‚Äî-]\s+", s, maxsplit=1)
    return parts[0].strip()

# --- –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–æ—Ü–∏–π ---

_STOP = {
    "–∏", "–≤", "–≤–æ", "–Ω–∞", "–ø–æ", "–∫", "–∫–æ", "–∏–∑", "–æ—Ç", "–¥–æ", "–æ", "–æ–±", "–æ–±–æ",
    "–ø—Ä–∏", "—Å–æ", "—Å", "–Ω–∞–¥", "–ø–æ–¥", "–∑–∞", "–¥–ª—è", "–±–µ–∑",
    "–º–æ–π", "–º–æ—è", "–º–æ—ë", "–º–æ–∏", "—Ç–≤–æ–π", "—Ç–≤–æ—è", "—Ç–≤–æ—ë", "—Ç–≤–æ–∏",
    "–µ–≥–æ", "–µ—ë", "–∏—Ö", "–Ω–∞—à", "–Ω–∞—à–∞", "–Ω–∞—à–µ", "–Ω–∞—à–∏", "–≤–∞—à", "–≤–∞—à–∞", "–≤–∞—à–µ", "–≤–∞—à–∏",
    "—Å–≤–æ–π", "—Å–≤–æ—è", "—Å–≤–æ—ë", "—Å–≤–æ–∏",
    "—ç—Ç–æ", "—ç—Ç–æ—Ç", "—ç—Ç–∞", "—ç—Ç–∏",
}

_PUNCT_RE = re.compile(r"[^\w\- ]+", re.UNICODE)

def _tokenize(s: str) -> List[str]:
    s = s.lower()
    s = _PUNCT_RE.sub(" ", s)
    toks = [t for t in s.split() if t and t not in _STOP]
    return toks

def _lemma(token: str) -> str:
    if not _MORPH:
        return token
    p = _MORPH.parse(token)
    if not p:
        return token
    return p[0].normal_form

def _pos(token: str) -> str:
    if not _MORPH:
        return ""
    p = _MORPH.parse(token)
    if not p:
        return ""
    return (p[0].tag.POS or "").upper()

def _normalize_emotion_phrase(phrase: str) -> str:
    """
    ¬´—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–≤–æ–∏—Ö —Å–∏–ª–∞—Ö¬ª -> ¬´—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å¬ª
    ¬´—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏¬ª -> ¬´—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ¬ª
    ¬´—Å–∏–ª–∞ –∏ —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ¬ª -> ¬´—Å–∏–ª–∞¬ª (–∫–∞–∫ –≥–ª–∞–≤–Ω–∞—è)
    """
    toks = _tokenize(phrase)
    if not toks:
        return ""

    lemmas = [(tok, _lemma(tok), _pos(tok)) for tok in toks]

    nouns = [lem for _, lem, pos in lemmas if pos == "NOUN"]
    if nouns:
        return max(nouns, key=len)

    adjs = [lem for _, lem, pos in lemmas if pos in {"ADJF", "ADJS"}]
    if adjs:
        return max(adjs, key=len)

    return lemmas[0][1]

def _normalize_emotions_list(emotions: List[str]) -> List[str]:
    out: List[str] = []
    seen = set()
    for e in emotions:
        norm = _normalize_emotion_phrase(e)
        if norm and norm not in seen:
            seen.add(norm)
            out.append(norm)
    return out

# --- –ø—É–±–ª–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–∞—Ä–∫–µ—Ä–æ–≤ ---

def extract_symbols_emotions(premium_html_or_text: str) -> Tuple[List[str], List[str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (symbols, emotions) –∏–∑ –ø—Ä–µ–º–∏—É–º-—Ä–∞–∑–±–æ—Ä–∞.
    –ù–∞ –≤—Ö–æ–¥ –º–æ–∂–Ω–æ –¥–∞—Ç—å Telegram-HTML (—Å <b>/<i>) ‚Äî –æ–Ω –±—É–¥–µ—Ç –æ—á–∏—â–µ–Ω.
    """
    text = _strip_tg_html(premium_html_or_text)

    # 1) –°–∏–º–≤–æ–ª—ã
    sym_block = _find_section(
        text,
        titles=[
            "–°–∏–º–≤–æ–ª—ã –∏ –º–æ—Ç–∏–≤—ã",
            "–°–∏–º–≤–æ–ª—ã",
            "–ö–ª—é—á–µ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã",
            "–û–±—Ä–∞–∑—ã –∏ —Å–∏–º–≤–æ–ª—ã",
        ],
    )
    symbols: List[str] = []
    if sym_block:
        for it in _split_bullets(sym_block):
            left = _left_before_dash(it)
            left = re.sub(r"^[\W_]+", "", left)  # —Å—Ä–µ–∑–∞–µ–º —ç–º–æ–¥–∑–∏/–º–∞—Ä–∫–µ—Ä—ã –≤ –Ω–∞—á–∞–ª–µ
            left = _clean_phrase(left)
            if left:
                symbols.append(left.lower())

    # 2) –≠–º–æ—Ü–∏–∏ (—Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç: ¬´–≠–º–æ—Ü–∏—è ‚Äî –ø–æ—è—Å–Ω–µ–Ω–∏–µ¬ª)
    emo_block = _find_section(
        text,
        titles=[
            "–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω",
            "–≠–º–æ—Ü–∏–∏",
            "–ß—É–≤—Å—Ç–≤–∞",
        ],
    )
    emotions: List[str] = []
    if emo_block:
        for it in _split_bullets(emo_block):
            left = _left_before_dash(it)
            left = re.sub(r"^[\W_]+", "", left)
            left = _clean_phrase(left)
            if not left:
                continue
            emotions.append(left.lower())

    # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–æ—Ü–∏–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π
    def _uniq(seq: List[str]) -> List[str]:
        seen = set()
        out: List[str] = []
        for x in seq:
            if x and x not in seen:
                seen.add(x)
                out.append(x)
        return out

    emotions = _normalize_emotions_list(emotions)

    return _uniq(symbols), _uniq(emotions)
